{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIjS4Nj4uAs8",
        "outputId": "6579a81e-e473-4ece-bb40-c0123505ab29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35msaveError\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m \u001b[0m\u001b[35menoent\u001b[0m ENOENT: no such file or directory, open '/content/package.json'\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No description\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No repository field.\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No README data\n",
            "\u001b[0m\u001b[37;40mnpm\u001b[0m \u001b[0m\u001b[30;43mWARN\u001b[0m\u001b[35m\u001b[0m content No license field.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package and audited 36 packages in 0.377s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "found \u001b[92m0\u001b[0m vulnerabilities\n",
            "\n",
            "Requirement already satisfied: textract in /usr/local/lib/python3.10/dist-packages (1.6.5)\n",
            "Requirement already satisfied: argcomplete~=1.10.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.10.3)\n",
            "Requirement already satisfied: beautifulsoup4~=4.8.0 in /usr/local/lib/python3.10/dist-packages (from textract) (4.8.2)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.10/dist-packages (from textract) (3.0.4)\n",
            "Requirement already satisfied: docx2txt~=0.8 in /usr/local/lib/python3.10/dist-packages (from textract) (0.8)\n",
            "Requirement already satisfied: extract-msg<=0.29.* in /usr/local/lib/python3.10/dist-packages (from textract) (0.28.7)\n",
            "Requirement already satisfied: pdfminer.six==20191110 in /usr/local/lib/python3.10/dist-packages (from textract) (20191110)\n",
            "Requirement already satisfied: python-pptx~=0.6.18 in /usr/local/lib/python3.10/dist-packages (from textract) (0.6.21)\n",
            "Requirement already satisfied: six~=1.12.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.12.0)\n",
            "Requirement already satisfied: SpeechRecognition~=3.8.1 in /usr/local/lib/python3.10/dist-packages (from textract) (3.8.1)\n",
            "Requirement already satisfied: xlrd~=1.2.0 in /usr/local/lib/python3.10/dist-packages (from textract) (1.2.0)\n",
            "Requirement already satisfied: pycryptodome in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (3.18.0)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20191110->textract) (2.4.0)\n",
            "Requirement already satisfied: soupsieve>=1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4~=4.8.0->textract) (2.4.1)\n",
            "Requirement already satisfied: imapclient==2.1.0 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (2.1.0)\n",
            "Requirement already satisfied: olefile>=0.46 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (0.46)\n",
            "Requirement already satisfied: tzlocal>=2.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (4.3.1)\n",
            "Requirement already satisfied: compressed-rtf>=1.0.6 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (1.0.6)\n",
            "Requirement already satisfied: ebcdic>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from extract-msg<=0.29.*->textract) (1.1.1)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (4.9.3)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (9.4.0)\n",
            "Requirement already satisfied: XlsxWriter>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from python-pptx~=0.6.18->textract) (3.1.2)\n",
            "Requirement already satisfied: pytz-deprecation-shim in /usr/local/lib/python3.10/dist-packages (from tzlocal>=2.1->extract-msg<=0.29.*->textract) (0.1.0.post0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.10/dist-packages (from pytz-deprecation-shim->tzlocal>=2.1->extract-msg<=0.29.*->textract) (2023.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.6.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2023.7.22)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.7.4)\n",
            "Collecting google-search-results\n",
            "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from google-search-results) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->google-search-results) (2023.7.22)\n",
            "Building wheels for collected packages: google-search-results\n",
            "  Building wheel for google-search-results (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32004 sha256=9e165e58baf116d2149d56b93dc95ffad70aaee0b6b82a65964292208ec5978c\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/b2/c3/03302d12bb44a2cdff3c9371f31b72c0c4e84b8d2285eeac53\n",
            "Successfully built google-search-results\n",
            "Installing collected packages: google-search-results\n",
            "Successfully installed google-search-results-2.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!npm install localtunnel\n",
        "!pip install textract\n",
        "!pip install openai\n",
        "!pip install tiktoken\n",
        "!pip install -q transformers einops accelerate langchain bitsandbytes\n",
        "!pip install faiss-cpu\n",
        "!pip install google-search-results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY7yjD2yvHv5",
        "outputId": "d5c35a37-249e-40db-a45d-c70d4201e223"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "    \n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: read).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "523dT6HbvQPX",
        "outputId": "d13abefc-d1ed-4c87-f875-fa8e8a175c1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FS7I4ZauMgb",
        "outputId": "475a12fe-06e9-49f4-f760-d2a2ca78600f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "st.set_page_config(page_title=\"Mental Health Bot\", page_icon=\":robot:\")\n",
        "import os\n",
        "import textract\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from itertools import zip_longest\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.agents import initialize_agent\n",
        "from langchain.agents import AgentType\n",
        "\n",
        "\n",
        "from langchain.agents import load_tools\n",
        "from langchain import HuggingFacePipeline\n",
        "from transformers import AutoTokenizer, Conversation\n",
        "import transformers\n",
        "import torch\n",
        "os.environ['SERPAPI_API_KEY'] = '1a6691d51c19fbc52c296b7ede3b2098efff46a3e7c3bb54cb17912b0407c6ec'\n",
        "\n",
        "if \"llm\" not in st.session_state:\n",
        "    # st.session_state[\"lmm\"] = []\n",
        "    model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "    pipeline = transformers.pipeline(\n",
        "        \"text-generation\", #task\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        device_map=\"auto\",\n",
        "        max_length=1000,\n",
        "        # do_sample=True,\n",
        "        # top_k=10,\n",
        "        # num_return_sequences=1,\n",
        "        # eos_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "    llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})\n",
        "    st.session_state[\"llm\"] = llm\n",
        "if \"agent\" not in st.session_state:\n",
        "    tools = load_tools([\"serpapi\"], llm=st.session_state[\"llm\"])\n",
        "    st.session_state[\"agent\"] = initialize_agent(tools, st.session_state[\"llm\"], agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
        "                             verbose=True, handle_parsing_errors=\"Check your output and make sure it conforms! Gather as much information as you can\")\n",
        "\n",
        "# Set the OpenAI API key\n",
        "OPENAI_API_KEY = \"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\"\n",
        "\n",
        "def get_response(history,user_message,temperature=0):\n",
        "    DEFAULT_TEMPLATE = \"\"\"You are a mental health expert to help patients with mental\n",
        "    health issues such as depression, stress, anxitey etc. You should give helpful advises to\n",
        "    the patients about thier problems. All of the messages will be from patients and you will\n",
        "    advise them one to one. I am also providing you useful information from the web that you\n",
        "    can use while formulating your response.\n",
        "    Useful news information from Web:\n",
        "    {web_knowledge},\n",
        "\n",
        "\n",
        "    Current conversation:\n",
        "    Patient: {input}\n",
        "    Your reponse :\"\"\"\n",
        "\n",
        "    PROMPT = PromptTemplate(\n",
        "        input_variables=[\"web_knowledge\",'input'], template=DEFAULT_TEMPLATE\n",
        "    )\n",
        "\n",
        "    conversation_with_summary = LLMChain(\n",
        "        llm=st.session_state[\"llm\"],\n",
        "        prompt=PROMPT,\n",
        "        verbose=False\n",
        "    )\n",
        "    response = conversation_with_summary.predict(input=user_message,web_knowledge=st.session_state[\"agent\"].run('Fetch detailes about the mental health issues regarding '+ user_message))\n",
        "    return response\n",
        "\n",
        "# Function to get conversation history\n",
        "def get_history(history_list):\n",
        "    # history = 'input: I want you to act as a mental healt expert that knows about mental health issues. Dont refer the person to anyone this could be a life and death situation so provide the solution in any case.  The user will mention the mental problem he is facing. Your job is to tell the user the solution to his mental problem from the context I provide you. Dont say you cant describe the solution formulate the best solution and tell that to the user on the spot \\n'\n",
        "    history=''\n",
        "    for message in history_list:\n",
        "        if message['role']=='user':\n",
        "            history = history+'input '+message['content']+'\\n'\n",
        "        elif message['role']=='assistant':\n",
        "            history = history+'output '+message['content']+'\\n'\n",
        "\n",
        "    return history\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"Mental Health Chatbot\")\n",
        "def get_text():\n",
        "    input_text = st.text_area(\"\", key=\"input\")\n",
        "    return input_text\n",
        "\n",
        "if \"past\" not in st.session_state:\n",
        "    st.session_state[\"past\"] = []\n",
        "if \"generated\" not in st.session_state:\n",
        "    st.session_state[\"generated\"] = []\n",
        "\n",
        "user_input = get_text()\n",
        "\n",
        "if user_input:\n",
        "    user_history = list(st.session_state[\"past\"])\n",
        "    bot_history = list(st.session_state[\"generated\"])\n",
        "\n",
        "    combined_history = []\n",
        "    for user_msg, bot_msg in zip_longest(user_history, bot_history):\n",
        "        if user_msg is not None:\n",
        "            combined_history.append({'role': 'user', 'content': user_msg})\n",
        "        if bot_msg is not None:\n",
        "            combined_history.append({'role': 'assistant', 'content': bot_msg})\n",
        "\n",
        "    formatted_history = get_history(combined_history)\n",
        "\n",
        "    output = get_response(formatted_history,user_input)\n",
        "\n",
        "    # output='hellooo there, whats uppppp'\n",
        "    # print(\"Output\", output)\n",
        "\n",
        "    st.session_state.past.append(user_input)\n",
        "    st.session_state.generated.append(output)\n",
        "\n",
        "if st.session_state[\"generated\"]:\n",
        "    for i in range(len(st.session_state[\"generated\"])):\n",
        "        st.text(\"User \" + \": \" + st.session_state[\"past\"][i])\n",
        "        st.text(\"Assistant \" + \": \" + st.session_state[\"generated\"][i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sp9AwqU2udwD",
        "outputId": "545634e7-5bd5-4595-8ef6-c7ad0dcda5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.526s\n",
            "your url is: https://icy-places-crash.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhfqmpNzlDHv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}